\Chapter{Background}

\Section{QMC}

\Subsection{Digital Sequence}

Talk about the whole idea briefly.\cite{hickernell2014reliable}

\Subsection{QMC}

Introduce QMC.

\Section{Control Variates}

\Subsection{A Brief Review}
Control variates has been know as variance reduction technique used in Monte Carlo methods.In this part we will brief review some crucial idea of this methods so we can see what's the problem for using it with QMC.\cite{hickernell2005control} 

Suppose we have the following integration approximation problem
\[I= \int_{[0,1]^d}f(x)dx\]

If we use Monte-Carlo method, the estimator should be 
\[
\hat{I}(f)=\frac{1}{n}\sum_{i=1}^{n}f(X_i), X_i\sim \mathcal{U}[0,1)^d
\]

Now suppose we have a known function $h$ and its value on the interval
$\int h(x)dx = \theta$. We construct a new estimator as the following 
\[ \hat{I}_{cv}(f)=\frac{1}{n}\sum_{i=1}^{n}\Big[ f(X_i)-\beta_{mc}[g(X_i)-\theta] \Big] \quad s.t.\; X_i\sim \mathcal{U}[0,1), \; i.i.d.\]

We can easily see it's an unbiased estimator, i.e. $\mathbb{E}(\hat{I}_{cv}) = I$. Now we want to pick the right $\beta_{mc}$ such that make the estimation more efficient. Base on previous MC error estimating formula~\eqref{}, we know it's achievable by minimizing the variance of the estimator, which is 
\begin{align*}
	\mathrm{Var}_{mc}(\hat{I}_{cv})
    =&\mathrm{Var}\Big( \frac{1}{n}\sum_{i=1}^{n}\big[ f(X_i)-\beta_{mc}[g(X_i)-\theta] \big]\Big)\\
    =&\frac{1}{n}\mathrm{Var}\Big(f(X_i)-\beta_{mc}[g(X_i)-\theta]\Big)\quad \text{by $X_i$ i.i.d} \\
    =&\frac{1}{n}\mathbb{E}\Big(\big[f(X_i)-\beta_{mc}[g(X_i)-\theta]-I\big]^2 \Big) \\
    =&\frac{1}{n}\mathbb{E}\Big(\big[ [f(X_i)-I] -\beta_{mc}[g(X_i)-\theta]\big]^2 \Big) \\
    =&\frac{1}{n}\mathbb{E}\big([f(X_i)-I]^2-2\beta_{mc}[f(X_i)-I][g(X_i)-\theta]+\beta_{mc}^2[g(X_i)-\theta]^2 \Big)\\
    =&\frac{1}{n}\Big(\mathrm{Var}[f(X_i)]-2\beta_{mc}\mathrm{Cov}[f(X_i),g(X_i)]+\beta_{mc}^2\mathrm{Var}[g(X_i)]\Big)\\
=&\frac{1}{n}\Big(\mathrm{Var}[g(X_i)](\beta_{mc}-\frac{\mathrm{Cov}[f(X_i),g(X_i)]}{\mathrm{Var}[g(X_i)]})^2+\mathrm{Var}[f(X_i]-\frac{\mathrm{Cov}^2[f(X_i),g(X_i)]}{\mathrm{Var}[g(X_i)]} \Big)
\end{align*}

Then the optimal $\beta_{mc}$ is given by 
\[
    \beta_{mc}=\frac{\mathrm{Cov}[f(X_i),g(X_i)]}{\mathrm{Var}[g(X_i)]}
\]

In which case the variance become
\[
    \mathrm{Var}_{mc}(\hat{I}_{cv})= \frac{\mathrm{Var}[f(X_i)]}{n}\big(1-\mathrm{corr}^2[f(X_i), g(X_i)]\big) 
\]

and we always have 
\[
\mathrm{Var}_{mc}(\hat{I}_{cv}) \leq \frac{\mathrm{Var}[f(X_i)]}{n}=\mathrm{Var}_{mc}(\hat{I})
\]

Now we can see the merit of control variates as a variance reduction method.In worst case, we get a completely uncorrelated $g$ that leads correlation to zero, and we have variance exactly the same as not using control variates. On the other hand, the more correlated our control variates is to the target function, the more variance we can get rid of by using the method.

\Subsection{Control Variates with QMC}
As we pointed out earlier, QMC use a different way for generating $X_i$, they are still identical(from same distribution) but not independent anymore, which caused the problem for control variates.

Suppose $X_1, \dots, X_n$ are generated by QMC rule, the estimator stays the same
\[
    \hat{I}_{cv}(f)=\frac{1}{n}\sum_{i=1}^{n}\Big[ f(X_i)-\beta_{rqmc}[g(X_i)-\theta] \Big] \quad X_i\in \mathcal{U}(0,1)
\]

We can easily prove it is still unbiased
\[
\mathbb{E}(\hat{I}_{cv})=\mathbb{E}\Big(\frac{1}{n}\sum_{i=1}^{n}\Big[f(X_i)-\beta_{mc}[g(X_i)-\theta] \Big] \Big)=I 
\]

However, it's not the same case as MC like we presented before, because we do not have i.i.d for $X_i$ this time
\[
\mathrm{Var}_{rqmc}(\hat{I}_{cv}) \not=\frac{1}{n}\mathrm{Var}\Big(f(X_i)-\beta_{mc}[g(X_i)-\theta]\Big)\\
\]

Instead the variance become
\begin{align*}
\mathrm{Var}_{rqmc}(\hat{I}_{cv})  
&=\mathrm{Var}\Big( \hat{I}- \beta_{rqmc}\hat{G}\Big)
    \quad s.t.\; \hat{I}=\sum_{i=1}^{n}f(X_i),\; \hat{G}=\sum_{i=1}^{n}[g(X_i)-\theta]\\
&=\mathrm{Var}(\hat{I})-2\beta_{rqmc}\mathrm{Cov}(\hat{I},\hat{G})+\beta_{rqmc}^2\mathrm{Var}(\hat{G})\\
&=\mathrm{Var}(\hat{G})\Big(\beta_{rqmc}-\frac{\mathrm{Cov}(\hat{I},\hat{G})}{\mathrm{Var}(\hat{G})}\Big)^2+\mathrm{Var}(\hat{I})-\frac{\mathrm{Cov}(\hat{I},\hat{G})}{\mathrm{Var}(\hat{G})}\\
\beta_{rqmc}^*&= \mathrm{Var} (\hat{G})^{-1}\mathrm{Cov} (\hat{I}, \hat{G})
\end{align*}

\Section{Reliable Adaptive QMC with digital sequence}

\Subsection{Idea of adaptive cubature algorithm}

One practical problem for QMC method is that how to get the sample size big enough for a required error tolerance. The idea in work of Hickernell and Jim√©nez Rugama(2014)\cite{hickernell2014reliable} is to construct a QMC algorithm with reliable error estimation on digital sequence. Here we briefly summarize their results.

The error of QMC method on digital sequence can be expressed in terms of Walsh coefficients of the integrand on certain cone conditions. 
\begin{align*}
&\Big|\int_{[0,1)^d}f(x)dx - \hat{I}_m(f)\Big| \leq a(r,m) \sum_{\lfloor 2^{m-r-1} \rfloor}^{2^{m-r}-1} |\hat{f}_{m,k}|\\
&\hat{I}_m(f): = \frac{1}{b^m}\sum_{i=0}^{b^m-1}f(z_i\oplus \Delta)\\
&\hat{f}_{m,k}=\text{ discrete Fourier coefficients of }f\\
&a(r,m) =\text{ inflation factor that depends on } \mathcal{C}
\end{align*}

Here is the cone condition.
\begin{align*}
	&\Big|\int_{[0,1)^d}f(x)dx - \hat{I}_m(f)\Big|
    \leq \sum {\bigcirc} 
	\leq \sum {\Box}
	\leq a(r,m) \sum_{\lfloor b^{m-r-1} \rfloor}^{b^{m-r}-1}|\hat{f}_{m,k}|\\
    &\bigcirc:= \sum_{\lambda=1}^{\infty}| \hat{f}_{\lambda b^m}|,\quad  
    \Box:= \sum_{\kappa=b^{l-1}}^{b^l-1}|\hat{f}_\kappa|,\quad
    \Diamond:=\sum_{\kappa=b^m}^{\infty}|\hat{f}_{\kappa}|\\
    &\mathcal{C}:=\Big\{\sum{\bigcirc} \leq \sum{\Diamond} \leq \sum{\Box}\Big\}
\end{align*}

\begin{figure}[h]
    \centering
    \caption{Cone condition for reliable adpative QMC algorithm}
    \includegraphics[width=0.8\textwidth]{figures/cone.bmp}
    \label{fg:cone}
\end{figure}
