\Chapter{Background}

\Section{Problem Setup}

\Section{Digital Sequence}


\Section{Control Variates}

Control variates has been know as variance reduction technique used in Monte Carlo methods. In this section we will brief review the ideas and some results of this methods.

Suppose we have the following integration approximation problem
\[I= \int_{[0,1)^d}f(x)dx\]

If we use Monte-Carlo method, the estimator should be 
\[
\hat{I}(f)=\frac{1}{n}\sum_{i=1}^{n}f(X_i), X_i\sim \mathcal{U}[0,1)^d
\]

 
Now suppose we have a known function $h$ and its value on the interval
$\int_{[0,1)^d} h(x)dx = \theta$.  We construct a new estimator as the following 
\[ \hat{I}_{cv}(f)=\frac{1}{n}\sum_{i=1}^{n}\Big[ f(X_i)-\beta_{mc}[h(X_i)-\theta] \Big] \quad s.t.\; X_i\sim \mathcal{U}[0,1), \; i.i.d.\]

We can easily see it's an unbiased estimator, i.e. $\mathbb{E}(\hat{I}_{cv}) = I$.
Now we want to pick the right $\beta_{mc}$ such that make the estimation more efficient.
For simplicity, we consider single control variate in this case.
Base on previous MC error estimating formula~\eqref{}, we know it's achievable by minimizing the variance of the estimator, which is 
\begin{align*}
	\mathrm{Var}_{mc}(\hat{I}_{cv})
    =&\mathrm{Var}\Big( \frac{1}{n}\sum_{i=1}^{n}\big[ f(X_i)-\beta_{mc}[h(X_i)-\theta] \big]\Big)\\
    =&\frac{1}{n}\mathrm{Var}\Big(f(X_i)-\beta_{mc}[h(X_i)-\theta]\Big)\quad \text{by $X_i$ i.i.d} \\
    =&\frac{1}{n}\mathbb{E}\Big(\big[f(X_i)-\beta_{mc}[h(X_i)-\theta]-I\big]^2 \Big) \\
    =&\frac{1}{n}\mathbb{E}\Big(\big[ [f(X_i)-I] -\beta_{mc}[h(X_i)-\theta]\big]^2 \Big) \\
    =&\frac{1}{n}\mathbb{E}\big([f(X_i)-I]^2-2\beta_{mc}[f(X_i)-I][h(X_i)-\theta]+\beta_{mc}^2[h(X_i)-\theta]^2 \Big)\\
    =&\frac{1}{n}\Big(\mathrm{Var}[f(X_i)]-2\beta_{mc}\mathrm{Cov}[f(X_i),h(X_i)]+\beta_{mc}^2\mathrm{Var}[h(X_i)]\Big)\\
=&\frac{1}{n}\Big(\mathrm{Var}[h(X_i)](\beta_{mc}-\frac{\mathrm{Cov}[f(X_i),h(X_i)]}{\mathrm{Var}[h(X_i)]})^2+\mathrm{Var}[f(X_i]-\frac{\mathrm{Cov}^2[f(X_i),h(X_i)]}{\mathrm{Var}[h(X_i)]} \Big)
\end{align*}

Then the optimal $\beta_{mc}$ is given by 
\begin{equation}
    \beta_{mc}^*=\frac{\mathrm{Cov}[f(X_i),h(X_i)]}{\mathrm{Var}[h(X_i)]}
    \label{eq:optBeta}
\end{equation}

In which case the variance become
\[
    \mathrm{Var}_{mc}(\hat{I}_{cv})= \frac{\mathrm{Var}[f(X_i)]}{n}\big(1-\mathrm{corr}^2[f(X_i), h(X_i)]\big) 
\]

and we always have 
\[
\mathrm{Var}_{mc}(\hat{I}_{cv}) \leq \frac{\mathrm{Var}[f(X_i)]}{n}=\mathrm{Var}_{mc}(\hat{I})
\]

Now we can see the merit of control variates as a variance reduction method.In worst case, we get a completely uncorrelated $g$ that leads correlation to zero, and we have variance exactly the same as not using control variates. On the other hand, the more correlated our control variates is to the target function, the more variance we can get rid of by using the method.

\Section{Reliable Adaptive QMC with digital sequence}

\Subsection{Idea of adaptive cubature algorithm}

One practical problem for QMC method is that how to get the sample size big enough for a required error tolerance. The idea in work of Hickernell and Jim√©nez Rugama(2014)\cite{hickernell2014reliable} is to construct a QMC algorithm with reliable error estimation on digital sequence. Here we briefly summarize their results.

The error of QMC method on digital sequence can be expressed in terms of Walsh coefficients of the integrand on certain cone conditions. 
\begin{align}
    \label{eq:errBound}
    &\text{ if } f \in \mathscr{C}\text{ then } \Big|\int_{[0,1)^d}f(x)dx - \hat{I}_m(f)\Big| \leq a(r,m) \sum_{\lfloor 2^{m-r-1} \rfloor}^{2^{m-r}-1} |\tilde{f}_{m,k}|\\
    &\hat{I}_m(f): = \frac{1}{b^m}\sum_{i=0}^{b^m-1}f(z_i\oplus \Delta)\notag\\
    &\tilde{f}_{m,k}=\text{ discrete Walsh coefficients of }f \notag\\
    &a(r,m) =\text{ inflation factor that depends on } \mathcal{C} \notag
\end{align}

Here is the defination of the cone condition.
\begin{align}
   &\mathscr{C}:=\Big\{f\in L^2[0,1)^d:\;\bigcirc \leq \hat{\omega}(m-l)\Diamond,\; l\leq m;\quad
   \Diamond \leq \mathring{\omega}(m-l) \Box, 
   \; l^*\leq l \leq m\Big\}\notag\\
   \label{eq:cone}
   &\bigcirc:= \sum_{\kappa=\lfloor b^{l-1} \rfloor}^{b^l-1} \sum_{\lambda=1}^{\infty}| \hat{f}_{\kappa+\lambda b^m}|,\quad  
   \Box:= \sum_{\kappa=b^{l-1}}^{b^l-1}|\hat{f}_\kappa|,\quad
   \Diamond:=\sum_{\kappa=b^m}^{\infty}|\hat{f}_{\kappa}|\\
   &l^*\in \mathbb{N}\text{ be fixed }; \forall m\in \mathbb{N},\hat{\omega}(m),\mathring{\omega}(m)\geq 0, \text{ and } \lim_{m\to \infty} \hat{\omega}(m)=0,\; \lim_{m\to \infty} \mathring{\omega}(m)=0\notag
\end{align}

The first inequality($\bigcirc \leq \Diamond$) means the sum of the larger indexed Walsh coefficients bounds a partial sum of the same coeffcients. Take $l=0, m=12$ for example, in Figure~\ref{fg:cone} the the sum of circles should be bounded by some factor times the sum of diamonds. The second inequality($\Diamond\leq \Box$) requires the sum of the larger Walsh coefficients be bounded by the sum of smaller indexed Walsh coefficients. Take $l=8$ at this time, which means in Figure~\ref{fg:cone} the sum of diamonds should be bounded by some relax factor times the squares.

The cone give some meanings for the functions about how they should behave to get the err bound formula~\eqref{eq:errBound}. This means that $|\hat{f}_\kappa|$ does not dramatically bounce back as $\kappa$ goes to infinity. Note that in Figure~\ref{fg:cone} we call circles the err bound, this is proven to be true and under the cone conditions we can estimate it using discrete Walsh coefficients instead of true Walsh coefficients.
\begin{figure}[h]
    \centering
    \caption{Cone condition for reliable adpative QMC algorithm}
    \includegraphics[width=0.7\textwidth]{figures/cone.bmp}
    \label{fg:cone}
\end{figure}
