\Chapter{Background}

\section{QMC}

\subsection{Digital Sequence}
Talk about the whole idea briefly.
\subsection{QMC}
Introduce QMC.


\section{Control Variates}

\subsection{A Brief Review}
Control variates has been know as variance reduction technique used in Monte Carlo methods.In this part we will brief review some crucial idea of this methods so we can see what's the problem for using it with QMC. \\ 
Suppose we have the following integration approximation problem:
\[I= \int_{[0,1]^d}f(x)dx\]
If we use Monte-Carlo method, the estimator should be: 
\[
\hat{I}(f)=\frac{1}{n}\sum_{i=1}^{n}f(X_i), X_i\sim \mathcal{U}[0,1)^d
\]
Now suppose we have a known function $h$ and its value on the interval
$\int h(x)dx = \theta$.
We construct a new estimator as the following: 
\[ \hat{I}_{cv}(f)=\frac{1}{n}\sum_{i=1}^{n}\Big( f(X_i)-\beta_{mc}(g(X_i)-\theta) \Big) \quad s.t. X_i\sim \mathcal{U}[0,1)\]
We can easily see it's an unbiased estimator.($\mathbb{E}(\hat{I}_{cv}) = I$)\\
Now we want to pick the right $\beta_{mc}$ such that make the estimation more efficient. Base on previous MC error estimating formula~\eqref{}, we know its achievable by minimizing the variance of the estimator, which is: 
\begin{align*}
	\quad &\mathrm{Var}_{mc}(\hat{I}_{cv}) \\
	=& \frac{\mathbb{E}\big([f(X_i)-\beta_{mc}(g(X_i)-\theta)-I]^2 \big)}{n} \\
	=& \frac{\mathbb{E}\big([(f(X_i)-I)^2-2\beta_{mc}(f(X_i)-I)(g(X_i)-\theta)-\beta_{mc}^2(g(X_i)-\theta)^2] \big)}{n}\\
	=& \frac{\mathrm{Var}(f(X_i)-2\beta_{mc}\mathrm{Cov}(f(X_i),g(X_i))+\beta_{mc}^2\mathrm{Var}(g(X_i)) }{n}\\
	=& \frac{\mathrm{Var}(g(X_i)(\beta_{mc}-\frac{\mathrm{Cov}(f(X_i),g(X_i))}{\mathrm{Var}(g(X_i))})^2+\mathrm{Var}(f(X_i)-\frac{\mathrm{Cov}^2(f(X_i),g(X_i))}{\mathrm{Var}(g(X_i))} }{n}
\end{align*}
then the optimal $\beta_{mc}$ is given by: 
$\beta_{mc}=\frac{\mathrm{Cov}(f(X_i),g(X_i))}{\mathrm{Var}(g(X_i))}$\\
In which case the variance become:
\[
\mathrm{Var}_{mc}(\hat{I}_{cv})= \frac{\mathrm{Var}(f(X_i)}{n}[1-\mathrm{corr}^2(f(X_i), g(X_i))]
\]


\subsection{Control Variates with QMC}

Suppose $X_1, \dots, X_n$ are generated by QMC rule, same estimator\\
We can prove it is still unbiased:
\[
\mathbb{E}(\hat{I}_{cv})=\mathbb{E}\Big(\frac{1}{n}\sum_{i=1}^{n}\Big( f(X_i)-\beta(g(X_i)-\theta)\Big)=I 
\]
However, it's not the same case for $\beta_{rqmc}$ since we do not have I.I.D. for $X_i$ this time
\begin{align*}
\mathrm{Var}_{rqmc}(\hat{I}_{cv}) &\not= \frac{\mathbb{E}\big([f(X_i)-\beta_{mc}(g(X_i)-\theta)-I]^2 \big)}{n}\\
&=\mathrm{Var}_{rqmc}\Big( \hat{I}- \beta_{rqmc}\hat{G}\Big)\quad , \hat{G}=\sum_{i=1}^{n}(g(X_i)-\theta)\\
\beta_{rqmc}^*&= \mathrm{Var} (\hat{G})^{-1}\mathrm{Cov} (\hat{G}, \hat{I})
\end{align*}


\section{Reliable Adaptive QMC with digital sequence}

\subsection{Idea of adaptive cubature algorithm}
\[
	\Big|\int_{[0,1)^d}f(x)dx - \hat{I}_m(f)\Big| \leq a(r,m) \sum_{\lfloor 2^{m-r-1} \rfloor}^{2^{m-r}-1} |\hat{f}_{m,k}|
\]
$\hat{I}_m(f): = \frac{1}{b^m}\sum_{i=0}^{b^m-1}f(z_i\oplus \Delta)$\\
$\hat{f}_{m,k}=$ discrete Fourier coefficients of $f$\\
$a(r,m) =$ inflation factor that depends on $\mathcal{C}$

\subsection{Error bound and the cone}
\begin{align*}
	&\Big|\int_{[0,1)^d}f(x)dx - \hat{I}_m(f)\Big|
    \leq \sum {\bigcirc} 
	\leq \sum {\Box}
	\leq a(r,m) \sum_{\lfloor b^{m-r-1} \rfloor}^{b^{m-r}-1}|\hat{f}_{m,k}|\\
    &\bigcirc:= \sum_{\lambda=1}^{\infty}| \hat{f}_{\lambda b^m}|,\quad  
    \Box:= \sum_{\kappa=b^{l-1}}^{b^l-1}|\hat{f}_\kappa|,\quad
    \Diamond:=\sum_{\kappa=b^m}^{\infty}|\hat{f}_{\kappa}|\\
    &\mathcal{C}:=\Big\{\sum{\bigcirc} \leq \sum{\Diamond} \leq \sum{\Box}\Big\}
\end{align*}
]
\includegraphics[width=0.8\textwidth]{figures/cone.bmp}
