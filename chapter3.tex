\Chapter{Reliable adaptive QMC with CV}

\Section{Idea to add control variates to reliable adaptive QMC}

The whole method starts with an idea similar to traditional control variates technique for MC.
If we know the integration of a function $\boldsymbol{h}=(h_1,\dots,h_J)$ on the interval same as our $f$, say $\int_{[0,1)^d}h_jdx=\theta_j$, then we can define a new function $g$:
\begin{align}\label{eq:defg}
    & g:=f-(\boldsymbol{h}-\boldsymbol{\theta})\boldsymbol{\beta}\\
    \notag
    &\text{s.t. }\boldsymbol{\theta}=(\theta_1,\dots,\theta_J),
    \boldsymbol{\beta}=(\beta_1,\dots,\beta_J)^T.
\end{align}
Then easily we can find that if we replace $f$ with $g$, the integration stays the same:
\[
	\int_{[0,1)^d}gdx
        =\int_{[0,1)^d}f-(\boldsymbol{h}-\boldsymbol{\theta})\boldsymbol{\beta}dx
			=\int_{[0,1)^d}fdx.
\]
Now we wonder if we can still use the same method as MC, the answer is no and the reason is in the next section. 

\Section{The problem of CV with random QMC}

The problem is that QMC is not a random process and we simply can't use the minimizing mean squre error trick as shown earlier anymore. However, one can use random QMC instead to `restore' the randomness to QMC \cite{}. 
Random QMC use a different way for generating $X_i$, they are still identical(i.e.\ from same distribution) but not independent, which will make it different from CV with MC.

Suppose $X_1, \dots, X_n$ are generated by QMC rule, the estimator stays the same
\[
    \hat{I}_{cv}(f)=\frac{1}{n}\sum_{i=1}^{n}\Big[ f(X_i)-\beta_{qmc}[h(X_i)-\theta] \Big] \quad X_i\in \mathcal{U}(0,1)
\]
We can easily prove it is still unbiased
\[
\mathbb{E}(\hat{I}_{cv})=\mathbb{E}\Big(\frac{1}{n}\sum_{i=1}^{n}\Big[f(X_i)-\beta_{mc}[h(X_i)-\theta] \Big] \Big)=I 
\]
However, it's not the same case as MC like we presented before, because we do not have i.i.d for $X_i$ this time
\[
\mathrm{Var}_{qmc}(\hat{I}_{cv}) \not=\frac{1}{n}\mathrm{Var}\Big(f(X_i)-\beta_{mc}[h(X_i)-\theta]\Big)\\
\]
Instead the variance become
\begin{align*}
    \mathrm{Var}\hat{I}_{cv})  
    &=\mathrm{Var}\Big( \hat{I}- \beta_{qmc}\hat{H}\Big)
    \quad s.t.\; \hat{I}=\sum_{i=1}^{n}f(X_i),\; \hat{H}=\sum_{i=1}^{n}[h(X_i)-\theta]\\
    &=\mathrm{Var}(\hat{I})-2\beta_{qmc}\mathrm{Cov}(\hat{I},\hat{H})+\beta_{qmc}^2\mathrm{Var}(\hat{H})\\
    &=\mathrm{Var}(\hat{H})\Big(\beta_{qmc}-\frac{\mathrm{Cov}(\hat{I},\hat{H})}{\mathrm{Var}(\hat{H})}\Big)^2+\mathrm{Var}(\hat{I})-\frac{\mathrm{Cov}(\hat{I},\hat{H})}{\mathrm{Var}(\hat{H})}
\end{align*}
The optimal $\beta_{qmc}$ is
\begin{equation}
    \beta_{qmc}^*= \mathrm{Var} (\hat{H})^{-1}\mathrm{Cov} (\hat{I}, \hat{H})
    \label{eq:optBetaqmc}
\end{equation}
which leave the variance to be
\[
    \mathrm{Var}_{qmc}(\hat{I}_{cv})=\mathrm{Var}(\hat{I})\big(1-\mathrm{corr}^2[\hat{I}, \hat{H}]\big)
\]

Now we are interested that if our previous formula for $\hat{\beta}_{mc}$ could be an estimation for $\hat{\beta}_{qmc}$. The fact is that they are generally not the same. Let's take the covariance part of formula ~\eqref{eq:optBetaqmc} and ~\eqref{eq:optBeta} to see the difference. 
\begin{align*}
    \mathrm{Cov}(\hat{I}, \hat{H})=&\int [f(X_1)+\dots+f(X_n)][h(X_1)+\dots+h(X_n)]\; d\mathbf{X}\\
    =&\int [\sum_{i=1}^{n}f(X_i)h(X_i) + \sum_{i,j=1}^{i\neq j}f(X_i)h(X_j)]\; d\mathbf{X}\\
    \neq&\int f(X_i)h(X_i)dX_i\\
    =&\mathrm{Cov}[(f(X_i),h(X_i)].
\end{align*}
There is also a very good example from Hickernell and Lemieux(2005)\cite{hickernell2005control}'s paper, showing that $\beta_{mc}$ and $\beta_{qmc}$ can make a quite different results in some cases. 


\Section{A new way to find $\beta$} 

As we stated in previous section, we can not find optimal $\beta$ by minimizing variance of estimator like MC. 
However, if using the reliable adaptive QMC method introduced in chapter 2, we may have another way to find $\beta$.

Recall equation~\eqref{eq:errBound} the error bound for new estimator of $g$ still holds
\begin{equation}\label{eq:qmccvErr}
	\Big|\int_{[0,1)^d}gdx - \hat{I}_m(g)\Big| \leq a(r,m) \sum_{\lfloor 2^{m-r-1} \rfloor}^{2^{m-r}-1} |\tilde{g}_{m,k}|
\end{equation}
Naturally, the new estimator become
\begin{equation}\label{eq:estcv}
    \hat{I}_m({g}): = \frac{1}{b^m}\sum_{i=0}^{b^m-1}g(z_i+\Delta)
\end{equation}


From~\eqref{eq:qmccvErr} it is clear that the optimal $\beta$ is the one that minimize the error term. 
\begin{align}
    \label{eq:optbeta1}
    \boldsymbol{\beta}^*
    &=\min_{\boldsymbol{\beta}}\sum_{\kappa=b^{m-r-1}}^{b^{m-r}-1} |\hat{g}_{\kappa}|\\
    \label{eq:optbeta2}
    &=\min_{\boldsymbol{\beta}}\sum_{\kappa=b^{m-r-1}}^{b^{m-r}-1}|\hat{f}_\kappa
    -(\hat{\boldsymbol{h}}_\kappa - \hat{\boldsymbol{\theta}})\boldsymbol{\boldsymbol{\beta}}|
    && \hat{\boldsymbol{h}}_\kappa=(\hat{h}_{\kappa,1},\dots, \hat{h}_{\kappa,J}),
    \hat{\boldsymbol{\theta}}=(\hat{\theta}_{\kappa,1},\dots,\hat{\theta}_{\kappa,J})\\
    \label{eq:optbeta3}
    &=\min_{\boldsymbol{\beta}}\|\hat{\boldsymbol{f}}-\hat{\boldsymbol{H}}\boldsymbol{\beta}\|_1
    && \hat{\boldsymbol{f}}= (\hat{f}_{b^{m-r-1}},\dots,\hat{f}_{b^{m-r}-1)})^T \\
    \label{eq:optbeta4}
    &\approx\min_{\boldsymbol{\beta}}\|\hat{\boldsymbol{f}}-\hat{\boldsymbol{H}}\boldsymbol{\beta}\|_2
    && \hat{\boldsymbol{H}}= (\hat{\boldsymbol{H}}_1, \dots, \hat{\boldsymbol{H}}_J)\\
    & \notag 
    &&\hat{\boldsymbol{H}_j}=(\hat{h}_{b^{m-r-1},j}-\hat{\theta}_j,\dots, \hat{h}_{b^{m-r}-1,j}-\hat{\theta}_j)^T.
\end{align}

The second equivalence is not hard to get, but the third one may not be so obvious. Let's consider it backwards. Suppose we have a vector A and it's $\mathcal{L}_1$-norm.
\[
   A=
    \begin{pmatrix}
        f_1-z_1\\
        f_2-z_2\\
        \hdots\\
        f_n-z_n
    \end{pmatrix},\quad
    \|A\|_1=\sum_{i=1}^{n}|f_i-z_i|, \quad
    z_i:=(\boldsymbol{h}_i-\boldsymbol{\theta})
\]

If we replace the index,$A$ is exactly what's inside the $\mathcal{L}_1$-norm in ~\eqref{eq:optbeta3}. 
Hence we justified the third equivalence. The reason we use an approximation instead, i.e. the $\mathcal{L}_1$-norm, is because there is no efficient way to solve it compared to existing least square methods.

\Section{The problem with $\theta$}

We noticed a problem in solution for optimal $\beta$, which is we do a lot of subtractions with $\theta$. This could be a large cost when we have difficult functions which means $b^{m-r}$ could be very large number. Therefore we present a way to avoid that part.

The idea is form a observation that Walsh transform of $\theta$ in~\eqref{qe:optbeta2} is actually zero, since $\hat{h}_\theta= \theta\delta_{\kappa,0}$ and the summation is not start from $\kappa=0$.

This simplifies~\eqref{eq:optbeta2} to the following. Note that we only need the information of function $f$ and $h$ to calculate $\beta^*$, $\theta$ has been get rid of the optimization process.
\begin{align}
    \boldsymbol{\beta}^*
    \label{eq:optbetanew}
    &=\min_{\boldsymbol{\beta}}\sum_{\kappa=b^{m-r-1}}^{b^{m-r}-1}|\hat{f}_\kappa
    -(\hat{\boldsymbol{h}}_\kappa - \hat{\boldsymbol{\theta}})\boldsymbol{\boldsymbol{\beta}}|\\
    &=\min_{\boldsymbol{\beta}}\sum_{\kappa=b^{m-r-1}}^{b^{m-r}-1}|\hat{f}_\kappa
    -(\hat{\boldsymbol{h}}_\kappa - \boldsymbol{\theta}\delta_{\kappa,0})\boldsymbol{\boldsymbol{\beta}}| \notag \\
    &=\min_{\boldsymbol{\beta}}\sum_{\kappa=b^{m-r-1}}^{b^{m-r}-1}|\hat{f}_\kappa
    -\hat{\boldsymbol{h}}_\kappa \boldsymbol{\boldsymbol{\beta}}|
    && \hat{\boldsymbol{f}}= (\hat{f}_{b^{m-r-1}},\dots,\hat{f}_{b^{m-r}-1)})^T \notag\\
    &=\min_{\boldsymbol{\beta}}\|\hat{\boldsymbol{f}}-\hat{\boldsymbol{H}}\boldsymbol{\beta}\|_1
    && \hat{\boldsymbol{H}}= (\hat{\boldsymbol{H}}_1, \dots, \hat{\boldsymbol{H}}_J)\notag \\
    &\approx\min_{\boldsymbol{\beta}}\|\hat{\boldsymbol{f}}-\hat{\boldsymbol{H}}\boldsymbol{\beta}\|_2
    &&\hat{\boldsymbol{H}_j}=(\hat{h}_{b^{m-r-1},j},\dots, \hat{h}_{b^{m-r}-1,j})^T \notag
\end{align}

The same problem happened with the estimator~\eqref{eq:estcv}. We have the similar solution for that.
\begin{align}
    \notag
    \hat{I}_m({g})
    & = \frac{1}{b^m}\sum_{i=0}^{b^m-1}g(z_i+\Delta)\\
    \notag
    & = \frac{1}{b^m}\sum_{i=0}^{b^m-1}f(z_i+\Delta)-(\boldsymbol{h}(z_i+\Delta)-\boldsymbol{\theta})\boldsymbol{\beta}\\
    \label{eq:estcvnew}
    & = \frac{1}{b^m}\sum_{i=0}^{b^m-1}[f(z_i+\Delta)-\boldsymbol{h}(z_i+\Delta)\boldsymbol{\beta}]+\boldsymbol{\theta}\boldsymbol{\beta}
\end{align}

After organize it the in format of~\eqref{eq:estcvnew}, $\theta$ is eliminated from the summation part. From these two parts of work on $\theta$ we managed to save $\frac{b-1}{b}b^{m-r}+b^m$ operations of subtraction.


\Section{The modified method}

Now we make the following changes:
\begin{align*}
    g&=f-\beta h\\ 
    \hat{I}_m({g})&= \frac{1}{b^m}\sum_{i=0}^{b^m-1}g(z_i+\Delta).
\end{align*}
And we have the following equivalence:
\begin{align*}
    \int_{[0,1)^d}fdx &= \int_{[0,1)^d}gdx +\theta\beta\\ 
    \hat{I}_m(f) &= \hat{I}_m(g)+\theta\beta.
\end{align*}
So the estimation error becomes:
\[
\Big| \int_{[0,1)^d}fdx- \hat{I}_m(f) \Big|
    =\Big| \int_{[0,1)^d}gdx- \hat{I}_m(g) \Big|.
\]
Here if our $g$ is in the cone we introduced earlier~\eqref{eq:cone}, then we can use the results from Hickernell and Jiménez Rugama(2014)\cite{hickernell2014reliable}, the error is bounded by:
\[
\Big|\int_{[0,1)^d}gdx - \hat{I}_m(g)\Big| \leq a(r,m) \sum_{\lfloor 2^{m-r-1} \rfloor}^{2^{m-r}-1} |\tilde{g}_{m,k}|.
\]
This leads to the same algorithm suggested by Hickernell and Jiménez Rugama(2014)\cite{hickernell2014reliable}, but since we are using control variates, several modifications have to be made.

\Section{The Algorithm}

We now give the algorithm for reliable adaptive QMC with control variates 
using digital sequence.
\begin{algorithm}[h]
\DontPrintSemicolon
\KwData{function $f$ and $\boldsymbol{H}$; 
    value of $\int_{[0,1)^d}h_jdx=\theta_j$; tolerance $\varepsilon$} 
    \KwResult{$\hat{I}(f)$; samples size; optimal $\beta$}
\Begin{
    \nl $m, r=$ start numbers, $x=$ $2^m$ sobolset points\;
    \nl \label{alg:alg1kappaFWT}
    get kappa map($\tilde{\kappa}$) and Walsh coefficients($\tilde{f},\tilde{\boldsymbol{H}})$ using algorithm~\ref{alg:kappaFWT} \;
    \nl \label{alg:alg1beta}
    $\boldsymbol{\beta}=
    \tilde{H}\big\{ \tilde{\kappa}[x(a:b)]\big\}\backslash
    \tilde{f}\big\{ \tilde{\kappa}[x(a:b)]\big\}, (a:b)=(2^{m-r-1}:2^{m-r}-1)$\;
    \nl 
    $g= f-\boldsymbol{H}\boldsymbol{\beta}$, repeat step~\ref{alg:alg1kappaFWT} on $g$\;
    \nl \label{alg:alg1stilde}
    $\tilde{S}_{m-r,m}(g)= \sum_{a}^{b}
    \Big| \tilde{g}\big\{ \tilde{\kappa}[x(a:b)]\big\}\Big|$ \;
    \nl \label{alg:alg1cone} 
    check whether $g$ is in the cone\;
    \nl \label{alg:alg1exit}     
    \If{$a(m,r)\tilde{S}_{m-r,m}(g)\leq \varepsilon$}
    {return $\hat{I}_m(g)=\sum_{i=0}^{2^m-1}f[x(i)]+\boldsymbol{\theta}\boldsymbol{\beta}$\;
    return $\boldsymbol{\beta},n=2^m$}
    \nl
    \For{$m=m+1:mmax$}
    {$xnext=$ next $2^{m-1}$ sobolset points\;    
        repeat step~\ref{alg:alg1kappaFWT} on $[x,xnext]$\;
        repeat step~\ref{alg:alg1stilde}, ~\ref{alg:alg1cone}, ~\ref{alg:alg1exit}\;
        }
    }
\caption{Reliable Adaptive QMC with control variates}\label{alg:qmccv}
\end{algorithm}

Note that for generating kappa map, i.e. step~\ref{alg:alg1kappaFWT} in Algorithm~\ref{alg:qmccv}, we used an explicit way to generate it.   
One can find the details for that in appendix from Hickernell and Llu{\'\i}s(2014) \cite{hickernell2014reliable}. Here we reorganize it and show it in algorithm ~\ref{alg:kappaFWT}.
Another important point need to be mentioned is that in our algorithm, we used an iterative way, which may require recalculating $beta$ for each iteration.
\begin{algorithm}
\DontPrintSemicolon
\KwData{function $f$; $\mathit{Y}_v^{(m)}$ ; $m \in \mathbb{N}_0$} 
    \KwResult{$\tilde{\kappa}$; $\tilde{S}_{m-r,m}(f)$}
\Begin{
    \If{$m=0$}
    {$\mathring{\mathbf{v}}(0) = 0$}
    \If{$m\geq1$}{\For{$m:1:-1$}{$\mathring{\mathbf{v}}_m(\mathbf{k})=\mathring{\mathbf{v}}_m-1(\mathbf{k})$\; $\mathring{\mathbf{v}}_m(\mathbf{k})=\mathbf{k},\; \mathbf{k}=b^{m-1}, \dots, b^m-1$}}
    \For{$l=m-1:max(1,m-r):-1$}
    {\For{$k=1:b^l-1$}
        {$\forall a \in \mathbb{F}_b$, find $a$ s.t. $|\mathit{Y}^{(m)}_{\mathring{\mathbf{v}}(k+a^*b^l)}| \geq |\mathit{Y}^{(m)}_{\mathring{\mathbf{v}}(k+ab^l)}|$}
    }
}
\caption{kappa map and discrete Walsh coeffcients}\label{alg:kappaFWT}
\end{algorithm}

\iffalse
\Section{When beta is not accurate?}

Note that in algorthm~\ref{alg:qmccv}, we didn't recalculate $\beta$ for every iteration. 
The reason is that for most functions this is not neccesary, but in certain case $\beta$ need to be updated to get the right answer. 
Here is an example showing that for certain strange functions beta needs to be updated. 

\fi
