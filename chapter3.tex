\Chapter{Reliable adaptive QMC with CV}

\Section{Idea to add control variates to reliable adaptive QMC}

The whole method starts with an idea similar to traditional control variates technique for MC.
If we know the integration of a function $\boldsymbol{h}=(h_1,\dots,h_J)$ on the interval same as our $f$, say $\int_{[0,1)^d}h_j\; \textrm{d}\mathbf{x}=\theta_j$, then we can define a new function $g$:
\begin{align}\label{eq:defg}
    & g:=f-(\boldsymbol{h}-\boldsymbol{\theta})\boldsymbol{\beta}\\
    \notag
    &\text{s.t. }\boldsymbol{\theta}=(\theta_1,\dots,\theta_J),
    \;\boldsymbol{\beta}=(\beta_1,\dots,\beta_J)^T.
\end{align}
Then easily we can find that if we replace $f$ with $g$, the integration stays the same
\[
    \int_{[0,1)^d}g\;\textrm{d}\mathbf{x}
        =\int_{[0,1)^d}f-(\boldsymbol{h}-\boldsymbol{\theta})\boldsymbol{\beta}\;\textrm{d}\mathbf{x}
            =\int_{[0,1)^d}f\;\textrm{d}\mathbf{x}.
\]
Now we wonder if we can still use the same method as MC. The answer is no, and the reason is in the next section. 

\Section{The problem of CV with randomized QMC}

The problem is that QMC is not a random process and we simply can't use the minimizing mean squre error trick as shown earlier anymore. However, one can use randomized QMC instead to `restore' the randomness to QMC \cite{owen2006warnock}. 
Randomized QMC use a different way for generating $\mathbf{X}_i$, they are still identical(i.e.\ from same distribution) but not independent, which will make it different from CV with MC.

Suppose $\{\mathbf{X}_1, \dots, \mathbf{X}_N\}$ are generated by QMC rule, the estimator stays the same
\[
    \hat{I}_{cv}(f)=\frac{1}{N}\sum_{i=1}^{N}\Big[ f(\mathbf{\mathbf{X}}_i)-\beta_{qmc}[h(\mathbf{X}_i)-\theta] \Big] \quad \mathbf{X}_i \text{ are Sobol points}.
\]
We can easily prove it is still unbiased
\[
\mathbb{E}(\hat{I}_{cv})=\mathbb{E}\Big(\frac{1}{N}\sum_{i=1}^{N}\Big[f(\mathbf{X}_i)-\beta_{qmc}[h(\mathbf{X}_i)-\theta] \Big] \Big)=I.
\]
However, it's not the same case as MC like we presented before, because we do not have IID sample points this time, i.e.
\[
    \mathrm{Var}(\hat{I}_{cv}) \not=\frac{1}{N}\mathrm{Var}\Big(f(\mathbf{X}_i)-\beta_{qmc}[h(\mathbf{X}_i)-\theta]\Big) \text{ \cite{hickernell2005control}}.
\]
Instead the variance becomes
\begin{align*}
    \mathrm{Var}(\hat{I}_{cv})  
    &=\mathrm{Var}\Big( \hat{I}- \beta_{qmc}\hat{H}\Big)
    \quad s.t.\; \hat{I}=\sum_{i=1}^{N}f(\mathbf{X}_i),\; \hat{H}=\sum_{i=1}^{N}[h(\mathbf{X}_i)-\theta]\\
    &=\mathrm{Var}(\hat{I})-2\beta_{qmc}\mathrm{Cov}(\hat{I},\hat{H})+\beta_{qmc}^2\mathrm{Var}(\hat{H})\\
    &=\mathrm{Var}(\hat{H})\Big(\beta_{qmc}-\frac{\mathrm{Cov}(\hat{I},\hat{H})}{\mathrm{Var}(\hat{H})}\Big)^2+\mathrm{Var}(\hat{I})-\frac{\mathrm{Cov}(\hat{I},\hat{H})}{\mathrm{Var}(\hat{H})}.
\end{align*}
The optimal $\beta_{qmc}$ is
\begin{equation}
    \beta_{qmc}^*= \mathrm{Var} (\hat{H})^{-1}\mathrm{Cov} (\hat{I}, \hat{H}),
    \label{eq:optBetaqmc}
\end{equation}
which leave the variance to be
\[
    \mathrm{Var}_{qmc}(\hat{I}_{cv})=\mathrm{Var}(\hat{I})\big(1-\mathrm{corr}^2[\hat{I}, \hat{H}]\big).
\]

Now we are interested that if our previous formula for $\hat{\beta}_{mc}$ could still be an estimation for $\hat{\beta}_{qmc}$. The fact is that they are generally not the same. Let's take the covariance part of formula~\eqref{eq:optBetaqmc} and formula~\eqref{eq:optBeta} to see the difference. 
\begin{align*}
    \mathrm{Cov}(\hat{I}, \hat{H})=&\int [f(\mathbf{X}_1)+\dots+f(\mathbf{X}_n)][h(\mathbf{X}_1)+\dots+h(\mathbf{X}_n)]\; d\mathbf{\mathbf{X}}\\
    =&\int [\sum_{i=1}^{N}f(\mathbf{X}_i)h(\mathbf{X}_i) + \sum_{i,j=1}^{i\neq j}f(\mathbf{X}_i)h(\mathbf{X}_j)]\; d\mathbf{\mathbf{X}}\\
    \neq&\int f(\mathbf{X}_i)h(\mathbf{X}_i)d\mathbf{X}_i\\
    =&\mathrm{Cov}[(f(\mathbf{X}_i),h(\mathbf{X}_i)].
\end{align*}
There is also a very good example from Hickernell and Lemieux (2005)\cite{hickernell2005control}'s paper, showing that $\beta_{mc}$ and $\beta_{qmc}$ can give a quite different results in some cases. 


\Section{A new way to find $\beta$} 

As we stated in previous section, we can not find optimal $\beta$ by minimizing variance of estimator like MC. 
However, if using the reliable adaptive QMC method introduced in chapter 2, we may have another way to find $\beta$.

Recall equation~\eqref{eq:errBound} the error bound for new estimator of $g$ still holds
\begin{equation}\label{eq:qmccvErr}
	\Big|\int_{[0,1)^d}gdx - \hat{I}_m(g)\Big| \leq a(r,m) \sum_{\lfloor 2^{m-r-1} \rfloor}^{2^{m-r}-1} |\tilde{g}_{m,k}|.
\end{equation}
Naturally, the new estimator becomes
\begin{equation}\label{eq:estcv}
    \hat{I}_m({g}): = \frac{1}{b^m}\sum_{i=0}^{b^m-1}g(z_i+\Delta).
\end{equation}


From~\eqref{eq:qmccvErr} it is clear that the optimal $\beta$ is the one that minimize the error term. 
\begin{align}
    \notag
    \boldsymbol{\beta}^*
    &=\min_{\boldsymbol{\beta}}\sum_{\kappa=b^{m-r-1}}^{b^{m-r}-1} |\hat{g}_{\kappa}|\\
    \label{eq:optbeta2}
    &=\min_{\boldsymbol{\beta}}\sum_{\kappa=b^{m-r-1}}^{b^{m-r}-1}|\hat{f}_\kappa
    -(\widehat{\boldsymbol{h}}_\kappa - \widehat{\boldsymbol{\theta}})\boldsymbol{\boldsymbol{\beta}}|
    && \widehat{\boldsymbol{h}}_\kappa=(\hat{h}_{\kappa,1},\dots, \hat{h}_{\kappa,J}),
    \widehat{\boldsymbol{\theta}}=(\hat{\theta}_{\kappa,1},\dots,\hat{\theta}_{\kappa,J})\\
    \label{eq:optbeta3}
    &=\min_{\boldsymbol{\beta}}\|\widehat{\boldsymbol{f}}-\widehat{\boldsymbol{H}}\boldsymbol{\beta}\|_1
    && \widehat{\boldsymbol{f}}= (\hat{f}_{b^{m-r-1}},\dots,\hat{f}_{b^{m-r}-1)})^T \\
    \label{eq:optbeta4}
    &\approx\min_{\boldsymbol{\beta}}\|\widehat{\boldsymbol{f}}-\widehat{\boldsymbol{H}}\boldsymbol{\beta}\|_2
    && \widehat{\boldsymbol{H}}= (\widehat{\boldsymbol{H}}_1, \dots, \widehat{\boldsymbol{H}}_J)\\
    & \notag 
    &&\widehat{\boldsymbol{H}_j}=(\hat{h}_{b^{m-r-1},j}-\hat{\theta}_j,\dots, \hat{h}_{b^{m-r}-1,j}-\hat{\theta}_j)^T.
\end{align}

The second equivalence~\eqref{eq:optbeta3} is not hard to get, but the third one~\eqref{eq:optbeta4} may not be so obvious. Let's consider it backwards. Suppose we have a vector A and it's $\mathcal{L}_1$-norm.
\[
   A=
    \begin{pmatrix}
        f_1-z_1\\
        f_2-z_2\\
        \hdots\\
        f_n-z_n
    \end{pmatrix},\quad
    \|A\|_1=\sum_{i=1}^{n}|f_i-z_i|, \quad
    z_i:=(\boldsymbol{h}_i-\boldsymbol{\theta})
\]

If we replace the index, $A$ is exactly what's inside the $\mathcal{L}_1$-norm in~\eqref{eq:optbeta3}. 
Hence, the third equivalence is justified.  
The reason we use the least square to approximate the $\mathcal{L}_1$ regression, is because it is less efficient to solve compared to existing least square methods.

\Section{The problem with $\theta$}

We noticed a problem in the solution for optimal $\beta$, which is that we do a lot of subtractions with $\theta$. This could be a large cost when we have difficult functions which means $b^{m-r}$ could be a very large number. Therefore we present a way to avoid that part.

The idea is form a observation that Walsh transform of $\theta$ in~\eqref{eq:optbeta2} is actually zero, since $\hat{h}_\theta= \theta\delta_{\kappa,0}$ for all nonzero $\kappa$ and the summation does not start from $\kappa=0$.

This simplifies~\eqref{eq:optbeta2} to the following. 
\begin{align}
    \boldsymbol{\beta}^*
    \notag
    &=\min_{\boldsymbol{\beta}}\sum_{\kappa=b^{m-r-1}}^{b^{m-r}-1}|\hat{f}_\kappa
    -(\hat{\boldsymbol{h}}_\kappa - \hat{\boldsymbol{\theta}})\boldsymbol{\boldsymbol{\beta}}|\\
    &=\min_{\boldsymbol{\beta}}\sum_{\kappa=b^{m-r-1}}^{b^{m-r}-1}|\hat{f}_\kappa
    -(\hat{\boldsymbol{h}}_\kappa - \boldsymbol{\theta}\delta_{\kappa,0})\boldsymbol{\boldsymbol{\beta}}| \notag \\
    &=\min_{\boldsymbol{\beta}}\sum_{\kappa=b^{m-r-1}}^{b^{m-r}-1}|\hat{f}_\kappa
    -\hat{\boldsymbol{h}}_\kappa \boldsymbol{\boldsymbol{\beta}}|\notag\\
    &=\min_{\boldsymbol{\beta}}\|\hat{\boldsymbol{f}}-\widehat{\boldsymbol{H}}\boldsymbol{\beta}\|_1\notag\\
    \label{eq:optbetanew}
    &\approx\min_{\boldsymbol{\beta}}\|\hat{\boldsymbol{f}}-\widehat{\boldsymbol{H}}\boldsymbol{\beta}\|_2
    &&\widehat{\boldsymbol{H}_j}=(\hat{h}_{b^{m-r-1},j},\dots, \hat{h}_{b^{m-r}-1,j})^T.
\end{align}
Note that we only need the information of function $f$ and $h$ to calculate $\beta^*$, $\theta$ has been get rid of the optimization process. Hence, $\widehat{\mathbf{H}i_j}$ is redefined as in \eqref{eq:optbetanew}. 

The same problem comes with the estimator~\eqref{eq:estcv}. We have the similar solution for that.
\begin{align}
    \notag
    \hat{I}_m({g})
    & = \frac{1}{b^m}\sum_{i=0}^{b^m-1}g(\mathbf{X_i})\\
    \notag
    & = \frac{1}{b^m}\sum_{i=0}^{b^m-1}f(\mathbf{X_i})-(\boldsymbol{h}(\mathbf{X_i})-\boldsymbol{\theta})\boldsymbol{\beta}\\
    \label{eq:estcvnew}
    & = \frac{1}{b^m}\sum_{i=0}^{b^m-1}[f(\mathbf{X_i})-\boldsymbol{h}(\mathbf{X_i})\boldsymbol{\beta}]+\boldsymbol{\theta}\boldsymbol{\beta}.
\end{align}

After organizing it in the same format as in~\eqref{eq:estcvnew}, $\theta$ is eliminated from the summation part. 
From these two revised formula~\eqref{eq:optbetanew} and~\eqref{eq:estcvnew}, we managed to save $(b-1)b^{m-r-1}+b^m$ operations of subtraction.


\Section{The modified method}

Now we make the following changes:
\begin{align*}
    g&:=f-\beta h\\ 
    \hat{I}_m({g})&:= \frac{1}{b^m}\sum_{i=0}^{b^m-1}g(\mathbf{X}_i).
\end{align*}
And we have the following equivalence:
\begin{align*}
    \int_{[0,1)^d}f\;\textrm{d}\mathbf{x} &= \int_{[0,1)^d}g\;\textrm{d}\mathbf{x} +\theta\beta\\ 
    \hat{I}_m(f) &= \hat{I}_m(g)+\theta\beta.
\end{align*}
So the estimation error becomes:
\[
    \Big| \int_{[0,1)^d}f\;\textrm{d}\mathbf{x}- \hat{I}_m(f) \Big|
    =\Big| \int_{[0,1)^d}g\;\textrm{d}\mathbf{x}- \hat{I}_m(g) \Big|.
\]
Here if our $g$ is in the cone we introduced earlier~\eqref{eq:cone}, then we can use the results from Hickernell and Jiménez Rugama(2014)\cite{hickernell2014reliable}, the error is bounded by:
\[
\Big|\int_{[0,1)^d}g\;\textrm{d}\mathbf{x} - \hat{I}_m(g)\Big| \leq a(r,m) \sum_{\lfloor 2^{m-r-1} \rfloor}^{2^{m-r}-1} |\tilde{g}_{m,\kappa}|.
\]
This leads to the same algorithm suggested by Hickernell and Jiménez Rugama(2014)\cite{hickernell2014reliable}, but since we are using control variates, several modifications have to be made.

\Section{The Algorithm}

We now provide the algorithm for reliable adaptive QMC with control variates 
using digital sequences.
\begin{algorithm}[h]
\DontPrintSemicolon
\KwData{function $f$ and $\boldsymbol{H}$; 
    value of $\int_{[0,1)^d}h_j\;\textrm{d}\mathbf{x}=\theta_j$; tolerance $\varepsilon$} 
    \KwResult{$\hat{I}(f)$; samples size; optimal $\beta$}
\Begin{
    \nl $m, r=$ start numbers, $x=$ $2^m$ sobolset points\;
    \nl \label{alg:alg1kappaFWT}
    get kappa map($\tilde{\kappa}$) and Walsh coefficients($\tilde{f},\tilde{\boldsymbol{H}})$ using algorithm~\ref{alg:kappaFWT} \;
    \nl \label{alg:alg1beta}
    $\boldsymbol{\beta}=
    \tilde{H}\big\{ \tilde{\kappa}[x(a:b)]\big\}\backslash
    \tilde{f}\big\{ \tilde{\kappa}[x(a:b)]\big\}, (a:b)=(2^{m-r-1}:2^{m-r}-1)$\;
    \nl 
    $g= f-\boldsymbol{H}\boldsymbol{\beta}$, repeat step~\ref{alg:alg1kappaFWT} on $g$\;
    \nl \label{alg:alg1stilde}
    $\tilde{S}_{m-r,m}(g)= \sum_{a}^{b}
    \Big| \tilde{g}\big\{ \tilde{\kappa}[x(a:b)]\big\}\Big|$ \;
    \nl \label{alg:alg1cone} 
    check whether $g$ is in the cone\;
    \nl \label{alg:alg1exit}     
    \If{$a(m,r)\tilde{S}_{m-r,m}(g)\leq \varepsilon$}
    {return $\hat{I}_m(g)=\sum_{i=0}^{2^m-1}f[x(i)]+\boldsymbol{\theta}\boldsymbol{\beta}$\;
    return $\boldsymbol{\beta},n=2^m$}
    \nl
    \For{$m=m+1:mmax$}
    {$xnext=$ next $2^{m-1}$ sobolset points\;    
        repeat step~\ref{alg:alg1kappaFWT} on $[x,xnext]$\;
        repeat step~\ref{alg:alg1stilde},~\ref{alg:alg1cone},~\ref{alg:alg1exit}\;
        }
    }
\caption{Reliable Adaptive QMC with control variates}\label{alg:qmccv}
\end{algorithm}

Note that for generating kappa map, i.e.\ step~\ref{alg:alg1kappaFWT} in Algorithm~\ref{alg:qmccv}, we used an explicit way to generate it.   
One can find the details for that in appendix from Hickernell and Jim{\'e}nez Rugama (2014)\cite{hickernell2014reliable}. Here we reorganize it and show it in algorithm~\ref{alg:kappaFWT}.
Another important point needed to be mentioned is that in our algorithm, we used an iterative way, which may require recalculating $beta$ at each iteration.
\begin{algorithm}
\DontPrintSemicolon
\KwData{function $f$; $\mathit{Y}_v^{(m)}$ ; $m \in \mathbb{N}_0$} 
    \KwResult{$\tilde{\kappa}$; $\tilde{S}_{m-r,m}(f)$}
\Begin{
    \If{$m=0$}
    {$\mathring{\mathbf{v}}(0) = 0$}
    \If{$m\geq1$}{\For{$m:1:-1$}{$\mathring{\mathbf{v}}_m(\mathbf{k})=\mathring{\mathbf{v}}_m-1(\mathbf{k})$\; $\mathring{\mathbf{v}}_m(\mathbf{k})=\mathbf{k},\; \mathbf{k}=b^{m-1}, \dots, b^m-1$}}
    \For{$l=m-1:max(1,m-r):-1$}
    {\For{$k=1:b^l-1$}
        {$\forall a \in \mathbb{F}_b$, find $a$ s.t. $|\mathit{Y}^{(m)}_{\mathring{\mathbf{v}}(k+a^*b^l)}| \geq |\mathit{Y}^{(m)}_{\mathring{\mathbf{v}}(k+ab^l)}|$}
    }
}
\caption{kappa map and discrete Walsh coefficients}\label{alg:kappaFWT}
\end{algorithm}

\iffalse
\Section{When beta is not accurate?}

Note that in algorithm~\ref{alg:qmccv}, we didn't recalculate $\beta$ for every iteration. 
The reason is that for most functions this is not necessary, but in certain case $\beta$ need to be updated to get the right answer. 
Here is an example showing that for certain strange functions beta needs to be updated. 

\fi
