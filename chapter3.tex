\Chapter{Reliable adaptive QMC with CV}

\Section{Idea to add CV to reliable adaptive QMC}

The whole method starts with an idea similar to traditional CV technique for MC.
Suppose we know the integral of a function $\boldsymbol{h}=(h_1,\dots,h_J)$ on the interval same as our $f$, say $\int_{[0,1)^d}h_j\; \textrm{d}\mathbf{x}=\theta_j$, then we can define a new function $g$:
\begin{align}\label{eq:defg}
    & g:=f-(\boldsymbol{h}-\boldsymbol{\theta})\boldsymbol{\beta}\\
    \notag
    &\text{s.t. }\boldsymbol{\theta}=(\theta_1,\dots,\theta_J),
    \;\boldsymbol{\beta}=(\beta_1,\dots,\beta_J)^T.
\end{align}
Then easily we can find that if we replace $f$ with $g$, the integration stays the same
\[
    \int_{[0,1)^d}g\;\textrm{d}\mathbf{x}
        =\int_{[0,1)^d}f-(\boldsymbol{h}-\boldsymbol{\theta})\boldsymbol{\beta}\;\textrm{d}\mathbf{x}
            =\int_{[0,1)^d}f\;\textrm{d}\mathbf{x}.
\]
Now we wonder if we can still use the exactly same method to compute CV coefficients as IID MC. The answer is ``no'', and the reason is in the next section. 

\Section{The problem of CV with randomized QMC}

The problem is that QMC is not a random process and we simply can't use the minimizing mean squre error trick as shown earlier anymore. However, one can use randomized QMC instead to ``restore'' the randomness to QMC \cite{owen2006warnock}. 
Randomized QMC use a different way for generating $\mathbf{X}_i$, they are still identically distributed but not independent, which makes the situation different from CV with IID MC.

Here we still use single CV to compare with the results from CV with IID MCin Chapter 2. 
Suppose $\{\mathbf{X}_1, \dots, \mathbf{X}_N\}$ are generated by QMC rule, the estimator stays the same
\[
    \ICV(f)=\frac{1}{N}\sum_{i=1}^{N}\Big[ f(\mathbf{\mathbf{X}}_i)-\BQMC [h(\mathbf{X}_i)-\theta] \Big] \quad \mathbf{X}_i \text{ are Sobol points}.
\]
We can easily prove it is still unbiased
\[
\mathbb{E}(\ICV)=\mathbb{E}\Big(\frac{1}{N}\sum_{i=1}^{N}\Big[f(\mathbf{X}_i)-\BQMC[h(\mathbf{X}_i)-\theta] \Big] \Big)=I.
\]
However, it's not the same case as MC like we presented before, because we do not have IID sample points this time, i.e.
\[
    \mathrm{Var}(\ICV) \not=\frac{1}{N}\mathrm{Var}\Big(f(\mathbf{X}_i)-\BQMC[h(\mathbf{X}_i)-\theta]\Big) \text{ \cite{hickernell2005control}}.
\]
Instead the variance becomes
\begin{align*}
    \mathrm{Var}(\ICV)  
    &=\mathrm{Var}\Big( \hat{I}- \BQMC \hat{H}\Big)
    \quad s.t.\; \hat{I}=\sum_{i=1}^{N}f(\mathbf{X}_i),\; \hat{H}=\sum_{i=1}^{N}[h(\mathbf{X}_i)-\theta]\\
    &=\mathrm{Var}(\hat{I})-2\BQMC \mathrm{Cov}(\hat{I},\hat{H})+\BQMC^2\mathrm{Var}(\hat{H})\\
    &=\mathrm{Var}(\hat{H})\Big(\BQMC -\frac{\mathrm{Cov}(\hat{I},\hat{H})}{\mathrm{Var}(\hat{H})}\Big)^2+\mathrm{Var}(\hat{I})-\frac{\mathrm{Cov}(\hat{I},\hat{H})}{\mathrm{Var}(\hat{H})}\text{ \cite{hickernell2005control}}.
\end{align*}
The optimal $\BQMC$ is
\begin{equation}
    \beta_{}^*= \mathrm{Var} (\hat{H})^{-1}\mathrm{Cov} (\hat{I}, \hat{H}) \text{ \cite{hickernell2005control}},
    \label{eq:optBetaqmc}
\end{equation}
which leave the variance to be
\[
    \mathrm{Var}(\hat{I}_{CV})=\mathrm{Var}(\hat{I})\big(1-\mathrm{corr}^2[\hat{I}, \hat{H}]\big) \text{ \cite{hickernell2005control}}.
\]

Now we are interested that if our previous formula for $\BMC^*$ could still be an estimation for $\BQMC^*$. The fact is that they are generally not the same. Let's take the covariance part of formula~\eqref{eq:optBetaqmc} and formula~\eqref{eq:optBeta} to see the difference. 
\begin{align*}
    \mathrm{Cov}(\hat{I}, \hat{H})=&\int [f(\mathbf{X}_1)+\dots+f(\mathbf{X}_n)][h(\mathbf{X}_1)+\dots+h(\mathbf{X}_n)]\; d\mathbf{\mathbf{X}}\\
    =&\int [\sum_{i=1}^{N}f(\mathbf{X}_i)h(\mathbf{X}_i) + \sum_{i,j=1}^{i\neq j}f(\mathbf{X}_i)h(\mathbf{X}_j)]\; d\mathbf{\mathbf{X}}\\
    \neq&\int f(\mathbf{X}_i)h(\mathbf{X}_i)d\mathbf{X}_i\\
    =&\mathrm{Cov}[(f(\mathbf{X}_i),h(\mathbf{X}_i)].
\end{align*}
There is also a very good example from Hickernell, Lemieux and Owen (2005)'s paper \cite{hickernell2005control}, showing that $\BMC^*$ and $\BQMC^*$ can be quite different in some cases. 

\newpage
\Section{A new way to find $\boldsymbol{\beta}$} 

As we stated in previous section, we can not find optimal $\boldsymbol{\beta}$ by minimizing variance of estimator like MC. 
However, if using the reliable adaptive QMC method introduced in chapter 2, we may have another way to find $\boldsymbol{\beta}$.

Recall equation~\eqref{eq:errBound} the error bound for new estimator of $g$ still holds
\begin{equation}\label{eq:qmccvErr}
	\Big|\int_{[0,1)^d}gdx - \hat{I}_m(g)\Big| \leq a(r,m) \sum_{\lfloor b^{m-r-1} \rfloor}^{b^{m-r}-1} |\tilde{g}_{m,k}|.
\end{equation}
Naturally, the new estimator becomes
\begin{equation}\label{eq:estcv}
    \hat{I}_m({g}): = \frac{1}{b^m}\sum_{i=0}^{b^m-1}g(\mathbf{X}_i).
\end{equation}


From~\eqref{eq:qmccvErr} it is clear that the optimal $\boldsymbol{\beta}$ is the one that minimize the error term. 
\begin{align}
    \notag
    \boldsymbol{\beta}^*
    &=\argmin_{\boldsymbol{\beta}}\sum_{\kappa=b^{m-r-1}}^{b^{m-r}-1} |\hat{g}_{\kappa}|
    && \quad \widehat{\boldsymbol{h}}_\kappa=(\hat{h}_{\kappa,1},\dots, \hat{h}_{\kappa,J})\\
    \label{eq:optbeta2}
    &=\argmin_{\boldsymbol{\beta}}\sum_{\kappa=b^{m-r-1}}^{b^{m-r}-1}|\hat{f}_\kappa -(\widehat{\boldsymbol{h}}_\kappa - \widehat{\boldsymbol{\theta}}_\kappa)\boldsymbol{\boldsymbol{\beta}}|
    && \quad \widehat{\boldsymbol{\theta}}_\kappa=(\hat{\theta}_{\kappa,1},\dots,\hat{\theta}_{\kappa,J})\\
    \notag
    & =\argmin_{\boldsymbol{\beta}}\|\widehat{\boldsymbol{f}}-\widehat{\boldsymbol{H}}\boldsymbol{\beta}\|_1
    && \quad \widehat{\boldsymbol{f}}= (\hat{f}_{b^{m-r-1}},\dots,\hat{f}_{b^{m-r}-1)})^T \\
    \notag
    & \approx\argmin_{\boldsymbol{\beta}}\|\widehat{\boldsymbol{f}}-\widehat{\boldsymbol{H}}\boldsymbol{\beta}\|_2
    && \quad \widehat{\boldsymbol{H}}= (\widehat{\boldsymbol{H}}_1, \dots, \widehat{\boldsymbol{H}}_J)\\
    & \notag 
    &&\quad \widehat{\boldsymbol{H}_j}=(\hat{h}_{b^{m-r-1},j}-\hat{\theta}_j,\dots, \hat{h}_{b^{m-r}-1,j}-\hat{\theta}_j)^T.
\end{align}

The reason we use the least square solution to approximate the $\mathcal{L}_1$ regression is that it is less efficient to solve $\mathcal{L}_1$ regression than least squre problems.

\Section{The problem with $\boldsymbol{\theta}$}

We noticed a problem in the solution for optimal $\boldsymbol{\beta}$, which is that we do a lot of subtractions with $\boldsymbol{\theta}$. 
This could be a large cost when we have difficult functions which means $b^{m-r}$ could be a very large number. Therefore we present a way to avoid that part.

The idea is form a observation that Walsh transform of $\boldsymbol{\theta}$ in~\eqref{eq:optbeta2} is actually zero, since $\hat{\boldsymbol{\theta}}_\kappa= \boldsymbol{\theta}\delta_{\kappa,0}$ for all nonzero $\kappa$ and the summation does not start from $\kappa=0$.

This simplifies~\eqref{eq:optbeta2} to the following. 
\begin{align}
    \boldsymbol{\beta}^*
    \notag
    &=\argmin_{\boldsymbol{\beta}}\sum_{\kappa=b^{m-r-1}}^{b^{m-r}-1}|\hat{f}_\kappa
    -(\hat{\boldsymbol{h}}_\kappa - \hat{\boldsymbol{\theta}}_\kappa)\boldsymbol{\boldsymbol{\beta}}|\\
    &=\argmin_{\boldsymbol{\beta}}\sum_{\kappa=b^{m-r-1}}^{b^{m-r}-1}|\hat{f}_\kappa
    -(\hat{\boldsymbol{h}}_\kappa - \boldsymbol{\theta}\delta_{\kappa,0})\boldsymbol{\boldsymbol{\beta}}| \notag \\
    &=\argmin_{\boldsymbol{\beta}}\sum_{\kappa=b^{m-r-1}}^{b^{m-r}-1}|\hat{f}_\kappa
    -\hat{\boldsymbol{h}}_\kappa \boldsymbol{\boldsymbol{\beta}}|\notag\\
    &=\argmin_{\boldsymbol{\beta}}\|\hat{\boldsymbol{f}}-\widehat{\boldsymbol{H}}\boldsymbol{\beta}\|_1\notag\\
    \label{eq:optbetanew}
    &\approx\argmin_{\boldsymbol{\beta}}\|\hat{\boldsymbol{f}}-\widehat{\boldsymbol{H}}\boldsymbol{\beta}\|_2
    &&\widehat{\boldsymbol{H}_j}=(\hat{h}_{b^{m-r-1},j},\dots, \hat{h}_{b^{m-r}-1,j})^T.
\end{align}
Note that we only need the information of function $f$ and $h$ to calculate $\boldsymbol{\beta}^*$, $\boldsymbol{\theta}$ has been get rid of the optimization process. 
Hence, $\widehat{\boldsymbol{H}_j}$ is redefined as in \eqref{eq:optbetanew}. 

The same problem comes with the estimator~\eqref{eq:estcv}. We have the similar solution for that.
\begin{align}
    \notag
    \hat{I}_m({g})
    & = \frac{1}{b^m}\sum_{i=0}^{b^m-1}g(\mathbf{X_i})\\
    \notag
    & = \frac{1}{b^m}\sum_{i=0}^{b^m-1}f(\mathbf{X_i})-(\boldsymbol{h}(\mathbf{X_i})-\boldsymbol{\theta})\boldsymbol{\beta}\\
    \label{eq:estcvnew}
    & = \frac{1}{b^m}\sum_{i=0}^{b^m-1}[f(\mathbf{X_i})-\boldsymbol{h}(\mathbf{X_i})\boldsymbol{\beta}]+\boldsymbol{\theta}\boldsymbol{\beta}.
\end{align}

After organizing it in the same format as in~\eqref{eq:estcvnew}, $\boldsymbol{\theta}$ is eliminated from the summation part. 
From these two revised formula~\eqref{eq:optbetanew} and~\eqref{eq:estcvnew}, we managed to save $(b-1)b^{m-r-1}+b^m$ operations of subtraction.

\newpage
\Section{The modified method}

Now we make the following changes:
\begin{align*}
    g&:=f-\boldsymbol{h}\boldsymbol{\beta}\\ 
    \hat{I}_m({g})&:= \frac{1}{b^m}\sum_{i=0}^{b^m-1}g(\mathbf{X}_i).
\end{align*}
And we have the following equivalence:
\begin{align*}
    \int_{[0,1)^d}f\;\textrm{d}\mathbf{x} &= \int_{[0,1)^d}g\;\textrm{d}\mathbf{x} +\boldsymbol{\theta\beta}\\ 
    \hat{I}_m(f) &= \hat{I}_m(g)+\boldsymbol{\theta\beta}.
\end{align*}
So the estimation error becomes:
\[
    \Big| \int_{[0,1)^d}f\;\textrm{d}\mathbf{x}- \hat{I}_m(f) \Big|
    =\Big| \int_{[0,1)^d}g\;\textrm{d}\mathbf{x}- \hat{I}_m(g) \Big|.
\]
Here if our $g$ is in the cone we introduced earlier~\eqref{eq:cone}, then we can use the results from Hickernell and Jiménez Rugama(2014)\cite{hickernell2014reliable}, the error is bounded by:
\[
\Big|\int_{[0,1)^d}g\;\textrm{d}\mathbf{x} - \hat{I}_m(g)\Big| \leq a(r,m) \sum_{\lfloor b^{m-r-1} \rfloor}^{b^{m-r}-1} |\tilde{g}_{m,\kappa}|.
\]
This leads to the same algorithm suggested by Hickernell and Jiménez Rugama(2014)\cite{hickernell2014reliable}, but since we are using control variates, several modifications have to be made.

\Section{The Algorithm}

We now provide the algorithm for reliable adaptive QMC with control variates 
using digital sequences.
\begin{algorithm}[h]
\DontPrintSemicolon
\KwData{function $f$ and $\boldsymbol{h}$; 
    value of $\int_{[0,1)^d}h_j\;\textrm{d}\mathbf{x}=\theta_j$; tolerance $\varepsilon$} 
    \KwResult{$\hat{I}(f)$; samples size; optimal $\beta$}
\Begin{
    \nl $m, r=$ start numbers, $x=$ $2^m$ sobolset points\;
    \nl \label{alg:alg1kappaFWT}
    get kappa map($\tilde{\kappa}$) and Walsh coefficients($\tilde{f},\tilde{\boldsymbol{h}})$ using algorithm~\ref{alg:kappaFWT} \;
    \nl \label{alg:alg1beta}
    $\boldsymbol{\beta}=
    \tilde{h}\big\{ \tilde{\kappa}[x(a:b)]\big\}\backslash
    \tilde{f}\big\{ \tilde{\kappa}[x(a:b)]\big\}, (a:b)=(2^{m-r-1}:2^{m-r}-1)$\;
    \nl 
    $g= f-\boldsymbol{h}\boldsymbol{\beta}$, repeat step~\ref{alg:alg1kappaFWT} on $g$\;
    \nl \label{alg:alg1stilde}
    $\tilde{S}_{m-r,m}(g)= \sum_{a}^{b}
    \Big| \tilde{g}\big\{ \tilde{\kappa}[x(a:b)]\big\}\Big|$ \;
    \nl \label{alg:alg1cone} 
    check if $g$ is not in the cone\;
    \nl \label{alg:alg1exit}     
    \If{$a(m,r)\tilde{S}_{m-r,m}(g)\leq \varepsilon$}
    {return $\hat{I}_m(g)=\sum_{i=0}^{2^m-1}f[x(i)]+\boldsymbol{\theta}\boldsymbol{\beta}$\;
    return $\boldsymbol{\beta},n=2^m$}
    \nl
    \For{$m=m+1:mmax$}
    {$xnext=$ next $2^{m-1}$ sobolset points\;    
        repeat step~\ref{alg:alg1kappaFWT} on $[x,xnext]$\;
        repeat step~\ref{alg:alg1stilde},~\ref{alg:alg1cone},~\ref{alg:alg1exit}\;
        }
    }
\caption{Reliable Adaptive QMC with control variates}\label{alg:qmccv}
\end{algorithm}

Note that for generating kappa map, i.e.\ step~\ref{alg:alg1kappaFWT} in Algorithm~\ref{alg:qmccv}, we used an explicit way to generate it.   
One can find the details for that in appendix from Hickernell and Jim{\'e}nez Rugama (2014)\cite{hickernell2014reliable}. Here we reorganize it and show it in algorithm~\ref{alg:kappaFWT}.
Another important point needed to be mentioned is that in our algorithm, we use an iterative way, which may require recalculating $\boldsymbol{\beta}$ at each iteration. 
\begin{algorithm}
\DontPrintSemicolon
\KwData{function $f$; $\mathit{Y}_v^{(m)}$ ; $m \in \mathbb{N}_0$} 
    \KwResult{$\tilde{\kappa}$; $\tilde{S}_{m-r,m}(f)$}
\Begin{
    \If{$m=0$}
    {$\mathring{\mathbf{v}}(0) = 0$}
    \If{$m\geq1$}{\For{$m:1:-1$}{$\mathring{\mathbf{v}}_m(\mathbf{k})=\mathring{\mathbf{v}}_m-1(\mathbf{k})$\; $\mathring{\mathbf{v}}_m(\mathbf{k})=\mathbf{k},\; \mathbf{k}=b^{m-1}, \dots, b^m-1$}}
    \For{$l=m-1:max(1,m-r):-1$}
    {\For{$k=1:b^l-1$}
        {$\forall a \in \mathbb{F}_b$, find $a$ s.t. $|\mathit{Y}^{(m)}_{\mathring{\mathbf{v}}(k+a^*b^l)}| \geq |\mathit{Y}^{(m)}_{\mathring{\mathbf{v}}(k+ab^l)}|$}
    }
}
\caption{kappa map and discrete Walsh coefficients}\label{alg:kappaFWT}
\end{algorithm}

\iffalse
\Section{When beta is not accurate?}

Note that in algorithm~\ref{alg:qmccv}, we didn't recalculate $\boldsymbol{\beta}$ for every iteration. 
The reason is that for most functions this is not necessary, but in certain case $\boldsymbol{\beta}$ need to be updated to get the right answer. 
Here is an example showing that for certain strange functions beta needs to be updated. 

\fi
