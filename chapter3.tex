\Chapter{Reliable Adaptive QMC Sobol with CV}
\Section{Theory}

\Subsection{Idea to add control variates to cubSobol}

The idea is similar to traditional control variates technique for Monte-Carlo.
If we know the integration of a function $\boldsymbol{h}=(h_1,\dots,h_J)$ on the interval same as our $f$, say $\int_{[0,1)^d}h_jdx=\theta_j$, then we can define a new function $g$
\begin{align}\label{eq:defg}
    & g:=f-(\boldsymbol{h}-\boldsymbol{\theta})\boldsymbol{\beta}\\
    \notag
    &\text{s.t. }\boldsymbol{\theta}=(\theta_1,\dots,\theta_J),
    \boldsymbol{\beta}=(\beta_1,\dots,\beta_J)^T
\end{align}

Then easily we can find that if we replace $f$ with $g$, the integration stays the same.
\[
	\int_{[0,1)^d}gdx
        =\int_{[0,1)^d}f-(\boldsymbol{h}-\boldsymbol{\theta})\boldsymbol{\beta}dx
			=\int_{[0,1)^d}fdx
\]

However, as we stated in chapter 3, we can not find optimal $\beta$ by minimizing variance of estimator like Monte-Carlo. Instead we have a different way to estimate error with adaptive QMC method introduced in chapter 2.

Recall equation~\eqref{} our error bound for the new estimator of $g$ still holds
\begin{equation}\label{eq:qmccvErr}
	\Big|\int_{[0,1)^d}gdx - \hat{I}_m(g)\Big| \leq a(r,m) \sum_{\lfloor 2^{m-r-1} \rfloor}^{2^{m-r}-1} |\hat{g}_{m,k}|
\end{equation}

Naturally, the new estimator become
\begin{equation}\label{eq:estcv}
    \hat{I}_m({g}): = \frac{1}{b^m}\sum_{i=0}^{b^m-1}g(z_i+\Delta)
\end{equation}

\Subsection{Optimizing $\beta$} 

From~\eqref{eq:qmccvErr} it is clear that the optimal $\beta$ is the one that minimize the error term. 
\begin{align}
    \label{eq:optbeta1}
    \boldsymbol{\beta}^*
    &=\min_{\boldsymbol{\beta}}\sum_{\kappa=b^{m-r-1}}^{b^{m-r}-1} |\hat{g}_{\kappa}|\\
    \label{eq:optbeta2}
    &=\min_{\boldsymbol{\beta}}\sum_{\kappa=b^{m-r-1}}^{b^{m-r}-1}|\hat{f}_\kappa
    -(\hat{\boldsymbol{h}}_\kappa - \hat{\boldsymbol{\theta}})\boldsymbol{\boldsymbol{\beta}}|
    && \hat{\boldsymbol{h}}_\kappa=(\hat{h}_{\kappa,1},\dots, \hat{h}_{\kappa,J}),
    \hat{\boldsymbol{\theta}}=(\hat{\theta}_{\kappa,1},\dots,\hat{\theta}_{\kappa,J})\\
    \label{eq:optbeta3}
    &=\min_{\boldsymbol{\beta}}\|\hat{\boldsymbol{f}}-\hat{\boldsymbol{H}}\boldsymbol{\beta}\|_1
    && \hat{\boldsymbol{f}}= (\hat{f}_{b^{m-r-1}},\dots,\hat{f}_{b^{m-r}-1)})^T \\
    \label{eq:optbeta4}
    &\approx\min_{\boldsymbol{\beta}}\|\hat{\boldsymbol{f}}-\hat{\boldsymbol{H}}\boldsymbol{\beta}\|_2
    && \hat{\boldsymbol{H}}= (\hat{\boldsymbol{H}}_1, \dots, \hat{\boldsymbol{H}}_J)\\
    & \notag 
    &&\hat{\boldsymbol{H}_j}=(\hat{h}_{b^{m-r-1},j}-\hat{\theta}_j,\dots, \hat{h}_{b^{m-r}-1,j}-\hat{\theta}_j)^T
\end{align}

The second equivalence is not hard to get, but the third one may not be so obvious. Let's consider it backwards. Suppose we have a vector A and it's $\mathcal{L}_1$-norm.
\[
   A=
    \begin{pmatrix}
        f_1-z_1\\
        f_2-z_2\\
        \hdots\\
        f_n-z_n
    \end{pmatrix},\quad
    \|A\|_1=\sum_{i=1}^{n}|f_i-z_i|, \quad
    z_i:=(\boldsymbol{h}_i-\boldsymbol{\theta})
\]

If we replace the index,$A$ is exactly what's inside the $\mathcal{L}_1$-norm in ~\eqref{eq:optbeta3}. 
Hence we justified the third equivalence. The reason we use an approximation instead, i.e. the $\mathcal{L}_1$-norm, is because there is no efficient way to solve it compared to existing least square methods.

\Subsection{The problem with $\theta$}

We noticed a problem in solution for optimal $\beta$, which is we do a lot of subtractions with $\theta$. This could be a large cost when we have difficult functions which means $b^{m-r}$ could be very large number. Therefore we present a way to avoid that part.

The idea is form a observation that Walsh transform of $\theta$ in~\eqref{qe:optbeta2} is actually zero, since $\hat{h}_\kappa = \hat{g}_\kappa-\theta\delta_{\kappa,0}$ and the summation is not start from $\kappa=0$.

This simplifies~\eqref{eq:optbeta3} to the following. Note that we only need the information of function $f$ and $h$ to calculate $\beta^*$, $\theta$ has been get rid of the optimization process.
\begin{align}
    \boldsymbol{\beta}^*
    \label{eq:optbetanew}
    &=\min_{\boldsymbol{\beta}}\|\hat{\boldsymbol{f}}-\hat{\boldsymbol{H}}\boldsymbol{\beta}\|_1
    && \hat{\boldsymbol{f}}= (\hat{f}_{b^{m-r-1}},\dots,\hat{f}_{b^{m-r}-1)})^T \\
        \label{eq:optbeta4}
    &\approx\min_{\boldsymbol{\beta}}\|\hat{\boldsymbol{f}}-\hat{\boldsymbol{H}}\boldsymbol{\beta}\|_2
    && \hat{\boldsymbol{H}}= (\hat{\boldsymbol{H}}_1, \dots, \hat{\boldsymbol{H}}_J)\\
    & \notag 
    &&\hat{\boldsymbol{H}_j}=(\hat{h}_{b^{m-r-1},j},\dots, \hat{h}_{b^{m-r}-1,j})^T
\end{align}

The same problem happened with the estimator~\ref{eq:estcv}.We have the similar solution for that.
\begin{align}
    \notag
    \hat{I}_m({g})
    & = \frac{1}{b^m}\sum_{i=0}^{b^m-1}g(z_i+\Delta)\\
    \notag
    & = \frac{1}{b^m}\sum_{i=0}^{b^m-1}f(z_i+\Delta)-(\boldsymbol{h}(z_i+\Delta)-\boldsymbol{\theta})\boldsymbol{\beta}\\
    \label{eq:estcvnew}
    & = \frac{1}{b^m}\sum_{i=0}^{b^m-1}[f(z_i+\Delta)-\boldsymbol{h}(z_i+\Delta)\boldsymbol{\beta}]+\boldsymbol{\theta}\boldsymbol{\beta}
\end{align}

After organize it the in format of~\eqref{eq:estcvnew}, $\theta$ is eliminated from the summation part. From these two parts of work on $\theta$ we managed to save $\frac{b-1}{b}b^{m-r}+b^m$ operations of subtraction.

\Section{Algorithm}

Now combining the work from~\ref{art:cubSobol} and our previous work, we have the following algorithm for reliable adaptive QMC with control variates.
\begin{algorithm}
\DontPrintSemicolon
\KwData{function $f$ and $\boldsymbol{H}$; 
    value of $\int_{[0,1)^d}h_jdx=\theta_j$; tolerance $\varepsilon$} 
\KwResult{estimate of $\int_{[0,1)^d}fdx$; samples size; optimal $\beta$}
\Begin{
    $m, r\longleftarrow$ start numbers\;
    $x\longleftarrow$ grab $2^m$ sobolset points\;
    \nl \label{alg:alg1kappa}
    $\tilde{\kappa}\longleftarrow$ get kappa map using~\ref{alg:kappamap}\;
    \nl \label{alg:alg1fwt}
    $\hat{f},\hat{\boldsymbol{H}}\longleftarrow $
        get Walsh transform of $f, \boldsymbol{h}$ using~\ref{alg:walshcoef}\;
    \nl \label{alg:alg1ab}
    $a\longleftarrow2^{m-r-1},b\longleftarrow2^{m-r}-1$\;
    \nl \label{alg:alg1beta}
    $\boldsymbol{\beta}\longleftarrow
    \hat{H}\big\{ \tilde{\kappa}[x(a:b)]\big\}\backslash
    \hat{f}\big\{ \tilde{\kappa}[x(a:b)]\big\},
    \hat{g}\longleftarrow \hat{f}-\boldsymbol{\hat{H}}\boldsymbol{\beta}$\;
    \nl \label{alg:alg1stilde}
    $\tilde{S}_{m-r,m}(g)\longleftarrow 
    \sum_{a}^{b}
    \Big| \hat{g}\big\{ \tilde{\kappa}[x(a:b)]\big\}\Big|,
    $ \;
    \nl \label{alg:alg1exit}
    \If{$a(m,r)\tilde{S}_{m-r,m}(g)\leq \varepsilon$}
    {return $\hat{I}_m(g)=\sum_{i=0}^{2^m-1}f[x(i)]+\boldsymbol{\theta}\boldsymbol{\beta}$\;
    return $\boldsymbol{\beta},n=2^m$}
    \nl
    \For{$m=m+1:mmax$}
    {$xnext\longleftarrow$ grab the next $2^{m-1}$ points in sobolset\;    
        repeat step~\ref{alg:alg1ab} with new $m$\;
        update kappa map(repeat step~\ref{alg:alg1kappa} with $(x:xnext)$)\; 
        repeat step~\ref{alg:alg1fwt} on $(x:xnext)$\;
        repeat stet~\ref{alg:alg1beta} with new $\tilde{\kappa},a,b$\;
        repeat step~\ref{alg:alg1stilde} with updated $\tilde{\kappa}$\;
        repeat step~\ref{alg:alg1exit} with new $\tilde{\kappa},\boldsymbol{\beta}$
        }
    }
\caption{Reliable Adaptive QMC with control variates}\label{alg:qmccv}
\end{algorithm}

\Section{Theorem}

To be added.
